# -*- coding: utf-8 -*-
"""projet data mining

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vg2rc8enpAs2njHo8fWWUiBmhjZFRRZQ

## Project Recipe Reviews

# Importation du dataset
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import re

from numpy.random import randint
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from scipy.stats import randint
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,r2_score, mean_squared_error
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

from wordcloud import WordCloud
from collections import Counter

# Charger le fichier CSV dans un DataFrame
df = pd.read_csv('Recipe Reviews and User Feedback Dataset.csv')
df = pd.DataFrame(df)
# Affiche les 5 premières lignes du DataFrame
df.head(5)

"""# Mise en Forme du dataset"""

#nettoyer la dataset
missing_values = df.isnull().sum()
print(missing_values)

# Trouver les lignes contenant des valeurs manquantes
rows_with_missing_values = df[df.isnull().any(axis=1)]
rows_with_missing_values

print( {df.duplicated().sum()})
df=df.drop_duplicates()
print( {df.duplicated().sum()})

# Remplacer les valeurs manquantes dans la colonne 'text' aux lignes 1507 et 2722 par 'unknown'
df.loc[[1507, 2722], 'text'] = df.loc[[1507, 2722], 'text'].fillna('unknown')
# Vérifier à nouveau s'il y a des valeurs manquantes
missing_values_after_handling = df.isnull().sum()
print(missing_values_after_handling)

# Trouver les lignes contenant des valeurs manquantes
rows_with_missing_values = df[df.isnull().any(axis=1)]
rows_with_missing_values

df = df.drop(['user_id', 'comment_id', 'created_at'], axis=1) #dropping redundant columns

"""Cleaning up non numerical values (except Text)"""

#find the non numerical values,

# Iterate through columns, excluding 'text'
for column in df.columns:
  if column != 'text':
    # Find non-numerical values in the current column
    non_numerical_values = df[~df[column].apply(lambda x: isinstance(x, (int, float)))]

    if not non_numerical_values.empty:
      print(f"Non-numerical values found in column '{column}':")
      print(non_numerical_values[column].unique())

"""# Feature engineering

### Processing the text
"""

import nltk
nltk.download("popular")
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import string
from textblob import TextBlob

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

def preprocess_text(text):
    if pd.isnull(text):  # Handle missing values
        return ""
    # Convert 'text' column to string type
    df['text'] = df['text'].astype(str)
    #replace empty text
    df["text"] = df["text"].fillna("")
    # Convert to lowercase
    text = text.lower()
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = re.sub(r'&#34;', '', text)
    #remove numbers
    text_without_numbers = re.sub(r'\d+', '', text)
    # Tokenize
    tokens = word_tokenize(text)
    # Remove stop words (including custom words)
    custom_stop_words = set(stopwords.words('english')).union({'used', 'use', 'make', 'made', 'recipe'})
    tokens = [word for word in tokens if word not in custom_stop_words]
    # Lemmatize
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return ' '.join(tokens)
    #Remove domain-specific noise
    text = re.sub(r'this recipe is', '', text)


df['processed_reviews'] = df['text'].apply(preprocess_text)
print(df[['text', 'processed_reviews']].head())

"""Generating a word cloud to identify key words"""

# Generate word cloud
all_words = ' '.join(df['processed_reviews'])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_words)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# Word frequency
word_counts = Counter(all_words.split())
print(word_counts.most_common(10))

"""### Creating dummy variables to count bad and good words in the text:"""

# Define lists of "good" and "bad" keywords based on domain knowledge
good_keywords = ["excellent", "perfect", "delicious", "amazing", "great", "love", "favorite", "tasty"]
bad_keywords = ["terrible", "bad", "disappointing", "horrible", "worst", "overcooked", "undercooked", "bland"]

# Function to flag the presence of keywords in text
def flag_keywords(text, keywords):
    text = str(text)
    return sum(1 for word in keywords if word in text.lower())

# Apply the function to the text column
df["good_keyword_count"] = df["text"].apply(lambda x: flag_keywords(x, good_keywords))
df["bad_keyword_count"] = df["text"].apply(lambda x: flag_keywords(x, bad_keywords))

# Create dummy variables for presence of any good or bad keywords
df["has_good_keywords"] = (df["good_keyword_count"] > 0).astype(int)
df["has_bad_keywords"] = (df["bad_keyword_count"] > 0).astype(int)

# Preview the updated DataFrame
print(df[["text", "good_keyword_count", "bad_keyword_count", "has_good_keywords", "has_bad_keywords"]].head())



"""### Adding sentiment score column"""

# Add a sentiment polarity column
df['sentiment_score'] = df['processed_reviews'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Classify reviews based on the score
def classify_sentiment(score):
    if score > 0:
        return 'Positive'
    elif score < 0:
        return 'Negative'
    else:
        return 'Neutral'

df['sentiment'] = df['sentiment_score'].apply(classify_sentiment)

#print(df[['text', 'sentiment_score', 'sentiment']].head())
recipe_sentiment_summary = df.groupby('recipe_name').agg(
    total_reviews=('processed_reviews', 'count'),
    avg_sentiment_score=('sentiment_score', 'mean'),
    positive_reviews=('sentiment', lambda x: (x == 'Positive').sum()),
    negative_reviews=('sentiment', lambda x: (x == 'Negative').sum()),
    neutral_reviews=('sentiment', lambda x: (x == 'Neutral').sum())
).reset_index()

print(recipe_sentiment_summary.head())

"""### Calculating a weighed score based on sentiment and stars for the recipes"""



stars_avg = df.groupby('recipe_name').agg(avg_stars=('stars', 'mean')).reset_index()
reviews_avg = df.groupby('recipe_name').agg( avg_sentiment_score=('sentiment_score', 'mean')).reset_index()
#print (stars_avg, reviews_avg)

#The average sentiment score ranges between -1 and +1, while star ratings are typically on a scale of 1–5. To combine them meaningfully, the sentiment score should be scaled to match the star ratings
df['rescaled_sentiment_score'] = ((df['sentiment_score'] + 1) / 2) * 4 + 1

sentiment_avg = df.groupby('recipe_name').agg(avg_rescaled_sentiment_score=('rescaled_sentiment_score', 'mean')).reset_index()
combined = pd.merge(stars_avg, sentiment_avg, on='recipe_name')

#we weigh the star score more heavily, as it is a fact wheras the sentiment score could be biased or less accurate.
combined['weighted_grade'] = 0.6 * combined['avg_stars'] + 0.4 * combined['avg_rescaled_sentiment_score']

print(combined.head())

print(df.columns)

"""# Descriptive Analysis of the dataset

## Univariate Statistics
"""

#Most frequent recipe
df['recipe_name'].value_counts()

for column in df.select_dtypes(include=['object']).columns:
    plt.figure(figsize=(8, 5))
    df[column].value_counts().head(10).plot(kind='bar')
    plt.title(f'Top Categories in {column}')
    plt.show()

plt.figure(figsize=(8, 6))
sns.countplot(x='stars', data=df, palette="mako")
plt.title('Count Plot of Star Ratings')
plt.xlabel('Star Rating')
plt.ylabel('Number of Reviews')
plt.show()

"""Most recipes are well rated, with 5 stars out of 5. Those with a rating of 0 are the recipes that haven’t received any . It will be interesting to study the recipes with low stars (1-3).

"""

# Count the number of users with a star rating of 0
zero_star_count = len(df[df['stars'] == 0])

# Display the result
print(f"Number of users who didn't vote (star rating of 0): {zero_star_count}")

# Sort and select top 5 comments
most_thumbs_up = df.sort_values(by='thumbs_up', ascending=False).head(5)
most_thumbs_down = df.sort_values(by='thumbs_down', ascending=False).head(5)

# Display top comments with the most thumbs up
print("Top 5 Comments with the Most Thumbs Up:\n")
for i, comment in enumerate(most_thumbs_up[['text', 'recipe_name', 'user_name', 'thumbs_up', 'thumbs_down']].itertuples(), start=1):
    print(f"{i}. Username: {comment.user_name}, Recipe: {comment.recipe_name}")
    print(f"   Comment: {comment.text}")
    print(f"   Thumbs Up: {comment.thumbs_up}, Thumbs Down: {comment.thumbs_down}\n")

# Display top comments with the most thumbs down
print("Top 5 Comments with the Most Thumbs Down:\n")
for i, comment in enumerate(most_thumbs_down[['text', 'recipe_name', 'user_name', 'thumbs_up', 'thumbs_down']].itertuples(), start=1):
    print(f"{i}. Username: {comment.user_name}, Recipe: {comment.recipe_name}")
    print(f"   Comment: {comment.text}")
    print(f"   Thumbs Up: {comment.thumbs_up}, Thumbs Down: {comment.thumbs_down}\n")

# Add numeric labels for the comments
most_thumbs_up['comment_number'] = range(1, len(most_thumbs_up) + 1)
most_thumbs_down['comment_number'] = range(1, len(most_thumbs_down) + 1)

# Plotting the top 5 comments with the most thumbs up
plt.figure(figsize=(10, 6))
sns.barplot(x=most_thumbs_up['comment_number'], y=most_thumbs_up['thumbs_up'], palette="viridis")
plt.title("Top 5 Comments with Most Thumbs Up", fontsize=16)
plt.ylabel("Thumbs Up", fontsize=14)
plt.xlabel("Comment Number", fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.show()

# Plotting the top 5 comments with the most thumbs down
plt.figure(figsize=(10, 6))
sns.barplot(x=most_thumbs_down['comment_number'], y=most_thumbs_down['thumbs_down'], palette="viridis")
plt.title("Top 5 Comments with Most Thumbs down", fontsize=16)
plt.ylabel("Thumbs down", fontsize=14)
plt.xlabel("Comment Number", fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.show()

# Calculate basic statistics for the 'stars' column, excluding 0 values
stars_no_zeros = df[df['stars'] != 0]['stars']
print("Stars column stats (excluding zeros):")
print(stars_no_zeros.describe())
print(f"Median of Stars (excluding zeros): {stars_no_zeros.median()}")

import plotly.express as px
fig = px.histogram(df, x="stars", color="recipe_name", nbins=6, title='recipes sorted by star rating')
fig.show()

for column in df.select_dtypes(include=['float64', 'int64']).columns:
    plt.figure(figsize=(8, 5))
    sns.histplot(df[column], kde=True, bins=30)
    plt.title(f'Distribution of {column}')
    plt.show()

"""Sentiment Analysis"""

from textblob import TextBlob

# Calculate sentiment polarity
df['sentiment'] = df['processed_reviews'].apply(lambda x: TextBlob(x).sentiment.polarity)
print(df[['text', 'sentiment']].head())

# Visualize sentiment distribution
plt.hist(df['sentiment'], bins=20, color='skyblue')
plt.title('Sentiment Distribution')
plt.xlabel('Polarity')
plt.ylabel('Frequency')
plt.show()

"""Most comments or reviews in the dataset might lean toward neutral or positive sentiment (clustered between 0 and 0.5).
The lower frequency of negative scores suggests that negative sentiments are relatively rare in the data.

## Bivariate statistics

### Correlation Matrix
"""

numeric_data = df.select_dtypes(include=['float64', 'int64']).columns

#Plot the correlation matrix as a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df[numeric_data].corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix')
plt.show()

correlation_matrix = df[numeric_data].corr()

"""top correlations"""

correlation_pairs = correlation_matrix.unstack()
threshold = 0.7

# Filter correlations that exceed the threshold
strong_correlations = correlation_pairs[correlation_pairs.abs() > threshold]

# Drop duplicates (each pair appears twice in the unstacked matrix)
strong_correlations = strong_correlations.drop_duplicates()

# Display strong correlations
print("Strong correlations (above threshold of 0.7):")
print(strong_correlations)

# Plot the filtered correlations
plt.figure(figsize=(10, 8))
sns.heatmap(
    correlation_matrix.loc[strong_correlations.index.get_level_values(0),
                           strong_correlations.index.get_level_values(1)],
    annot=True, cmap='coolwarm', vmin=-1, vmax=1
)
plt.title(f'Strong Correlations (|corr| > {threshold})')
plt.show()

"""# Machine Learning: Classification model - Logistic Regression

"""

# Create binary target variable: 1 for "good recipes" (4 or 5 stars), 0 for "bad recipes" (1-3 stars)
df["good_recipe"] = df["stars"].apply(lambda x: 1 if x >= 4 else 0)

#Text vectorization using TF-IDF
tfidf = TfidfVectorizer(stop_words="english", max_features=5000)  # Limit features for performance
X_text = tfidf.fit_transform(df["text"])
X = X_text
y = df["good_recipe"]

#Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

#Evaluate the model
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Accuracy Score:", accuracy_score(y_test, y_pred))

#Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["Predicted Bad", "Predicted Good"],
            yticklabels=["Actual Bad", "Actual Good"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

"""The model achieves strong overall accuracy (87%) but struggles with "bad recipes" (class 0), showing low recall (15%). It performs well in identifying "good recipes" (class 1) with high precision (87%) and recall (99%).

The confusion matrix reveals the model's strong performance in predicting "good recipes," with 3,083 true positives and only 26 false negatives. However, it struggles with "bad recipes," correctly identifying only 78 out of 528 while misclassifying 450 as good recipes. This highlights the class imbalance and the difficulty in detecting less frequent "bad recipes.

# Regression Model - Linear Regression

### Linear regression
"""

X = df[['thumbs_up', 'thumbs_down', 'good_keyword_count', 'bad_keyword_count', 'sentiment_score', 'rescaled_sentiment_score']]
y = df['stars']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
print(f"R-squared: {r2_score(y_test, y_pred)}")
print(f"Mean Squared Error: {mean_squared_error(y_test, y_pred)}")

"""R2: only 7.88% of the variance in the stars (target variable) is explained by the features in the model, which is low. It suggests that the chosen features (reply_count, thumbs_up, thumbs_down, user_reputation, sentiment_score) are not strongly predictive of the recipe ratings. There may be missing key predictors or a non-linear relationship that the model cannot capture.

An MSE of 2.17 suggests that predictions deviate significantly from the actual ratings, which are on a 1-5 scale.

While it is hard to judge this value without a benchmark, the high MSE aligns with the low R-squared, indicating room for improvement in model performance.
"""

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)  # Plot actual vs. predicted values
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2) # Add a diagonal line for reference
plt.xlabel("Actual Star Ratings")
plt.ylabel("Predicted Star Ratings")
plt.title("Actual vs. Predicted Star Ratings (Linear Regression)")
plt.show()

# Residual plot
residuals = y_test - y_pred
plt.figure(figsize=(10, 6))
sns.residplot(x=y_pred, y=residuals)
plt.xlabel("Predicted Star Ratings")
plt.ylabel("Residuals")
plt.title("Residual Plot")
plt.show()

"""### random forest"""

vectorizer = TfidfVectorizer(max_features=500)  # Limit features if needed
df['text'] = df['text'].fillna('')
X = vectorizer.fit_transform(df['text'])
y = df['stars']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf_regressor = RandomForestRegressor(random_state=42, n_estimators=100, max_depth=10, n_jobs=-1)
rf_regressor.fit(X_train, y_train)

# Make predictions on the test data
y_pred = rf_regressor.predict(X_test)

# Evaluate the model
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

print(f'R² Score: {r2:.4f}')
print(f'Mean Squared Error: {mse:.4f}')

"""similar results, still bad.

# hyperparameter

Sadly after many attempts, I can't execute these commands, they take hours so I don't have the outputs.

hyperparameter tuning for regression
"""

rf = RandomForestRegressor(random_state=42)

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best parameters and model evaluation
best_rf = grid_search.best_estimator_
print(f"Best Parameters: {grid_search.best_params_}")

y_pred = best_rf.predict(X_test)
print("R-squared:", r2_score(y_test, y_pred))
print("Mean Squared Error:", mean_squared_error(y_test, y_pred))

"""hyperparameter tuning for classification"""

X = df['text']
y = df['stars']

vectorizer = TfidfVectorizer(max_features=500)
X = vectorizer.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# SMOTE for oversampling the minority class (bad recipes)
smote = SMOTE(sampling_strategy='minority', random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)

# Hyperparameter tuning with GridSearchCV
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [10, 20],
    'min_samples_split': [2],
    'min_samples_leaf': [1, 2]
}

grid_search = GridSearchCV(rf_model, param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(X_train_resampled, y_train_resampled)

# Get the best model
best_rf_model = grid_search.best_estimator_
# Predict on the test set
y_pred = best_rf_model.predict(X_test)

print("Classification Report:")
print(classification_report(y_test, y_pred))
print(f"Accuracy Score: {accuracy_score(y_test, y_pred)}")

# confusion matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Bad Recipes", "Good Recipes"], yticklabels=["Bad Recipes", "Good Recipes"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Examine feature importance (keywords contributing to classification)
feature_names = tfidf.get_feature_names_out()
coefficients = model.coef_[0]
top_positive = np.argsort(coefficients)[-10:]
top_negative = np.argsort(coefficients)[:10]

print("Top Positive Features (Good Recipes):")
for index in top_positive:
    print(f"{feature_names[index]}: {coefficients[index]}")

print("\nTop Negative Features (Bad Recipes):")
for index in top_negative:
    print(f"{feature_names[index]}: {coefficients[index]}")